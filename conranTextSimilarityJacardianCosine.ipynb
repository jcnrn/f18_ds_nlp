{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "conranTextSimilarityJacardianCosine.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jcnrn/f18_ds_nlp/blob/master/conranTextSimilarityJacardianCosine.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "0XY1PYXCSQGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Homework 2"
      ]
    },
    {
      "metadata": {
        "id": "s4msEUMsSQGP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Introduction: The purpose of this homework will be to examine the similarity of a number of articles contained in the data directory. Specifically, the intent is to implement different document similarity techniques and then see how similar these documents are using said techniques. Finally, we would like to present the similarity in a table or heatmap."
      ]
    },
    {
      "metadata": {
        "id": "R6HviuarSQGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 1:\n",
        "In order to begin this exercise, we will first need to iterate over the data directory and read the content of each file. We will store the data from each file in a hash."
      ]
    },
    {
      "metadata": {
        "id": "rg3gwuFsSQGa",
        "colab_type": "code",
        "colab": {},
        "outputId": "b157b3f3-ad83-468d-b54d-735f52e850dc"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk.data\n",
        "import numpy\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "import itertools\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "import re, math\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(os.getcwd())\n",
        "os.chdir('C:/Users/JEC/Desktop/hw2/data/')\n",
        "print(os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\JEC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\JEC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\JEC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "C:\\Users\\JEC\\Desktop\\hw2\n",
            "C:\\Users\\JEC\\Desktop\\hw2\\data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SPnV5a1YSQGv",
        "colab_type": "code",
        "colab": {},
        "outputId": "da6de3a8-547a-4ea5-dbb3-f21ac08adbe7"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\JEC\\\\Desktop\\\\hw2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "oEbQjiXVSQHC",
        "colab_type": "code",
        "colab": {},
        "outputId": "dfdcb860-c800-4010-bb64-ba5b26c20c93"
      },
      "cell_type": "code",
      "source": [
        "#path_to_homework_5_data_directory = \"C:/Users/JEC-Work/Desktop/hw2/data\"\n",
        "\n",
        "#article_hash = {} # this hash should serve to represent the content of the files in the data directory\n",
        "# use the filename as the hash key and the value will be the text of the file\n",
        "# thus you would be able to retrieve an individual documents text like: article_hash[\"article_1\"]\n",
        "\n",
        "# here we will get a list of the filenames of things contained in the data directory\n",
        "#files = [f for f in listdir(path_to_homework_5_data_directory) if isfile(join(path_to_homework_5_data_directory, f))]\n",
        "\n",
        "# here you will iterate over all the files contained in the directory\n",
        "#for f in files:\n",
        "#        file_texts += read_file(join(directory, f) )\n",
        "\n",
        "# implement the rest of this line and the content of the loop\n",
        "    # you will read the content of the file and put the text into the hash\n",
        "    # article_hash[filename_variable] = the content of the file\n",
        "\n",
        "\n",
        "\n",
        "#get list of files in directory\n",
        "file = []\n",
        "file = os.listdir(\"C:/Users/JEC/Desktop/hw2/data\")\n",
        "\n",
        "#create list of text from files\n",
        "text = []\n",
        "for i in file:\n",
        "    text.append(open(i, encoding='utf-8').read())\n",
        "\n",
        "#create dictionary of the two\n",
        "article_hash= dict(zip(file,text))\n",
        "#article_hash['article_9']                        #test\n",
        "article_hash.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['article_1', 'article_10', 'article_11', 'article_12', 'article_2', 'article_3', 'article_4', 'article_5', 'article_6', 'article_7', 'article_8', 'article_9'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "ZbD8lXe8SQHU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 2:\n",
        "Text processing. Now that you have the content of the files read into a hash you will be able to process them. Specifically, you should perhaps employ sentence segmentation, tokenization, and stemming to get a new representation of the document. Here you will want to build a sufficiently flexible approach so that you can try out several different pre-processing strategies to see how it affects your similarity scores.\n",
        "\n",
        "We'll create a new hash that contains the processed text."
      ]
    },
    {
      "metadata": {
        "id": "VAWP9dmNSQHZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "75a8c3c5-af7a-4433-a04c-6d0d20976166"
      },
      "cell_type": "code",
      "source": [
        "# setup a new hash to store the results in\n",
        "processed_article_hash = {}\n",
        "\n",
        "# iterate through the keys, i.e. document ids, in the hash to pull out the stored text and process\n",
        "for key in article_hash.keys():\n",
        "    text_of_article = article_hash[key]\n",
        "    \n",
        "    text_of_article = treebank_tokenizer.tokenize(text_of_article)\n",
        "    text_of_article = [word for word in text_of_article if word not in stopwords.words('english')]\n",
        "    text_of_article = [wordnet_lemmatizer.lemmatize(word) for word in text_of_article]\n",
        "    \n",
        "    # here you can apply your segmentation, tokenization, and stemming steps as you see fit\n",
        "    \n",
        "    processed_article_hash[key] = text_of_article\n",
        "    \n",
        "processed_article_hash[\"article_12\"]    #test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Two',\n",
              " 'D.C.',\n",
              " 'Metro',\n",
              " 'employee',\n",
              " 'charged',\n",
              " 'stealing',\n",
              " 'thousand',\n",
              " 'fare',\n",
              " 'http',\n",
              " ':',\n",
              " '//www.facebook.com/dana.hedgpeth',\n",
              " '8-10',\n",
              " 'minute',\n",
              " 'Metro',\n",
              " 'employee',\n",
              " 'Horace',\n",
              " 'Dexter',\n",
              " 'McDade',\n",
              " 'typically',\n",
              " 'spent',\n",
              " 'day',\n",
              " 'traveling',\n",
              " 'one',\n",
              " 'subway',\n",
              " 'station',\n",
              " 'another',\n",
              " ',',\n",
              " 'collecting',\n",
              " 'bill',\n",
              " 'coin',\n",
              " 'fare',\n",
              " 'machines.',\n",
              " 'Often',\n",
              " ',',\n",
              " 'Metro',\n",
              " 'Transit',\n",
              " 'Police',\n",
              " 'officer',\n",
              " 'John',\n",
              " 'Vincent',\n",
              " 'Haile',\n",
              " 'nearby',\n",
              " ',',\n",
              " 'watching',\n",
              " 'McDade',\n",
              " 'made',\n",
              " 'round',\n",
              " 'trucked',\n",
              " 'cash',\n",
              " 'collection',\n",
              " 'facility',\n",
              " 'Alexandria.',\n",
              " 'But',\n",
              " 'authority',\n",
              " 'allege',\n",
              " 'two',\n",
              " 'men',\n",
              " 'taking',\n",
              " 'unofficial',\n",
              " 'detour',\n",
              " ',',\n",
              " 'driving',\n",
              " 'spot',\n",
              " 'near',\n",
              " 'Capital',\n",
              " 'Beltway',\n",
              " 'hundred',\n",
              " 'yard',\n",
              " 'collection',\n",
              " 'building',\n",
              " 'hiding',\n",
              " 'part',\n",
              " 'day',\n",
              " '’',\n",
              " 'take.',\n",
              " 'When',\n",
              " 'shift',\n",
              " 'done',\n",
              " ',',\n",
              " 'would',\n",
              " 'return',\n",
              " 'car',\n",
              " 'make',\n",
              " 'money.',\n",
              " 'On',\n",
              " 'Wednesday',\n",
              " 'night',\n",
              " ',',\n",
              " 'federal',\n",
              " 'authority',\n",
              " 'arrested',\n",
              " 'pair',\n",
              " 'charged',\n",
              " 'conspiring',\n",
              " 'commit',\n",
              " 'theft.',\n",
              " 'According',\n",
              " 'affidavit',\n",
              " 'filed',\n",
              " 'U.S.',\n",
              " 'District',\n",
              " 'Court',\n",
              " ',',\n",
              " 'two',\n",
              " 'conspiring',\n",
              " 'since',\n",
              " 'least',\n",
              " '2010.',\n",
              " 'Surveillance',\n",
              " 'tape',\n",
              " 'confidential',\n",
              " 'source',\n",
              " 'revealed',\n",
              " 'Haile',\n",
              " 'used',\n",
              " 'bag',\n",
              " 'collected',\n",
              " 'coin',\n",
              " 'buy',\n",
              " 'lottery',\n",
              " 'ticket',\n",
              " ',',\n",
              " 'according',\n",
              " 'affidavit.',\n",
              " 'He',\n",
              " 'collected',\n",
              " 'close',\n",
              " '$',\n",
              " '63,000',\n",
              " 'winning',\n",
              " 'past',\n",
              " 'four',\n",
              " 'years.',\n",
              " 'A',\n",
              " 'federal',\n",
              " 'judge',\n",
              " 'released',\n",
              " 'McDade',\n",
              " ',',\n",
              " '58',\n",
              " ',',\n",
              " 'Bowie',\n",
              " 'Haile',\n",
              " ',',\n",
              " '54',\n",
              " ',',\n",
              " 'Woodbridge',\n",
              " 'bond',\n",
              " 'made',\n",
              " 'initial',\n",
              " 'appearance',\n",
              " 'Thursday',\n",
              " 'afternoon.',\n",
              " 'A',\n",
              " 'status',\n",
              " 'conference',\n",
              " 'set',\n",
              " 'Monday.',\n",
              " 'McDade',\n",
              " 'told',\n",
              " 'Magistrate',\n",
              " 'Judge',\n",
              " 'T.',\n",
              " 'Rawles',\n",
              " 'Jones',\n",
              " 'Jr.',\n",
              " 'plan',\n",
              " 'hire',\n",
              " 'lawyer.',\n",
              " 'Haile',\n",
              " 'asked',\n",
              " 'judge',\n",
              " 'appoint',\n",
              " 'one',\n",
              " ',',\n",
              " 'according',\n",
              " 'federal',\n",
              " 'officials.',\n",
              " 'If',\n",
              " 'convicted',\n",
              " ',',\n",
              " 'McDade',\n",
              " 'Haile',\n",
              " 'face',\n",
              " 'maximum',\n",
              " 'penalty',\n",
              " 'five',\n",
              " 'year',\n",
              " 'prison.',\n",
              " '“',\n",
              " 'They',\n",
              " 'stealing',\n",
              " 'system',\n",
              " '’',\n",
              " 'entrusted',\n",
              " 'protect',\n",
              " ',',\n",
              " 'defend',\n",
              " 'support',\n",
              " ',',\n",
              " '”',\n",
              " 'Neil',\n",
              " 'H.',\n",
              " 'MacBride',\n",
              " ',',\n",
              " 'U.S.',\n",
              " 'attorney',\n",
              " 'Eastern',\n",
              " 'District',\n",
              " 'Virginia',\n",
              " ',',\n",
              " 'said.',\n",
              " 'It',\n",
              " 'unclear',\n",
              " 'much',\n",
              " 'money',\n",
              " 'may',\n",
              " 'stolen.',\n",
              " 'Prosecutors',\n",
              " 'say',\n",
              " 'Haile',\n",
              " '’',\n",
              " 'bank',\n",
              " 'record',\n",
              " 'show',\n",
              " 'unexplained',\n",
              " 'cash',\n",
              " 'deposit',\n",
              " '$',\n",
              " '150,000',\n",
              " 'since',\n",
              " '2008',\n",
              " 'probe',\n",
              " 'ongoing.',\n",
              " 'The',\n",
              " 'pair',\n",
              " 'systematically',\n",
              " 'worked',\n",
              " 'station',\n",
              " 'Metro',\n",
              " '’',\n",
              " 'rail',\n",
              " 'line',\n",
              " 'Maryland',\n",
              " ',',\n",
              " 'District',\n",
              " 'Virginia',\n",
              " ',',\n",
              " 'according',\n",
              " 'official',\n",
              " 'involved',\n",
              " 'case',\n",
              " 'spoke',\n",
              " 'condition',\n",
              " 'anonymity',\n",
              " 'authorized',\n",
              " 'discus',\n",
              " 'publicly.',\n",
              " 'Investigators',\n",
              " 'found',\n",
              " 'Haile',\n",
              " 'would',\n",
              " 'often',\n",
              " 'switch',\n",
              " 'security',\n",
              " 'assignment',\n",
              " 'officer',\n",
              " 'could',\n",
              " 'work',\n",
              " 'McDade.',\n",
              " 'According',\n",
              " 'affidavit',\n",
              " ',',\n",
              " 'surveillance',\n",
              " 'team',\n",
              " 'observed',\n",
              " 'McDade',\n",
              " 'Haile',\n",
              " 'three',\n",
              " 'day',\n",
              " 'December',\n",
              " 'January.',\n",
              " 'The',\n",
              " 'two',\n",
              " 'would',\n",
              " 'ride',\n",
              " 'parking',\n",
              " 'lot',\n",
              " 'Marriott',\n",
              " 'Courtyard',\n",
              " 'hotel',\n",
              " 'Eisenhower',\n",
              " 'Avenue',\n",
              " 'underpass',\n",
              " 'hide',\n",
              " 'bag',\n",
              " 'money',\n",
              " 'would',\n",
              " 'unload',\n",
              " 'Metro',\n",
              " 'van.',\n",
              " 'Later',\n",
              " ',',\n",
              " 'man',\n",
              " 'would',\n",
              " 'return',\n",
              " 'separately',\n",
              " '—',\n",
              " 'Haile',\n",
              " 'gray',\n",
              " 'Jaguar',\n",
              " 'McDade',\n",
              " 'green',\n",
              " 'Ford',\n",
              " '—',\n",
              " 'get',\n",
              " 'stashed',\n",
              " 'cash',\n",
              " 'go',\n",
              " 'separate',\n",
              " 'way',\n",
              " ',',\n",
              " 'according',\n",
              " 'court',\n",
              " 'record',\n",
              " 'involved',\n",
              " 'case.',\n",
              " 'Investigators',\n",
              " 'placed',\n",
              " 'Global',\n",
              " 'Positioning',\n",
              " 'System',\n",
              " 'tracking',\n",
              " 'device',\n",
              " 'car',\n",
              " 'Metro',\n",
              " 'van.',\n",
              " 'The',\n",
              " 'underpass',\n",
              " 'quarter-mile',\n",
              " 'Metro',\n",
              " '’',\n",
              " 'revenue-collection',\n",
              " 'facility',\n",
              " 'opposite',\n",
              " 'direction',\n",
              " 'Haile',\n",
              " '’',\n",
              " 'route',\n",
              " 'home',\n",
              " 'Woodbridge',\n",
              " ',',\n",
              " 'authority',\n",
              " 'said.',\n",
              " 'Haile',\n",
              " 'used',\n",
              " 'stolen',\n",
              " 'money',\n",
              " 'buy',\n",
              " 'Virginia',\n",
              " 'Lottery',\n",
              " 'ticket',\n",
              " '—',\n",
              " 'sometimes',\n",
              " 'paying',\n",
              " 'bag',\n",
              " 'change',\n",
              " ',',\n",
              " 'authority',\n",
              " 'say.',\n",
              " 'Between',\n",
              " 'October',\n",
              " 'December',\n",
              " ',',\n",
              " 'used',\n",
              " '$',\n",
              " '28,000',\n",
              " 'coin',\n",
              " 'cash',\n",
              " 'purchase',\n",
              " 'ticket',\n",
              " ',',\n",
              " 'according',\n",
              " 'affidavit',\n",
              " 'filed',\n",
              " 'Metro',\n",
              " 'Transit',\n",
              " 'Police',\n",
              " 'Capt.',\n",
              " 'Kevin',\n",
              " 'P.',\n",
              " 'Gaddis.',\n",
              " 'Gaddis',\n",
              " 'wrote',\n",
              " 'affidavit',\n",
              " 'FBI',\n",
              " 'tipped',\n",
              " 'alleged',\n",
              " 'scheme',\n",
              " 'September',\n",
              " ',',\n",
              " 'learned',\n",
              " 'man',\n",
              " 'dressed',\n",
              " 'police',\n",
              " 'uniform',\n",
              " 'routinely',\n",
              " 'bought',\n",
              " 'several',\n",
              " 'hundred',\n",
              " 'dollar',\n",
              " '’',\n",
              " 'worth',\n",
              " 'scratch-off',\n",
              " 'lottery',\n",
              " 'ticket',\n",
              " 'Woodbridge',\n",
              " 'store.',\n",
              " 'A',\n",
              " 'source',\n",
              " 'later',\n",
              " 'told',\n",
              " 'agent',\n",
              " 'officer',\n",
              " 'going',\n",
              " 'store',\n",
              " 'three',\n",
              " 'year',\n",
              " ',',\n",
              " 'first',\n",
              " 'carrying',\n",
              " '“',\n",
              " 'could',\n",
              " 'hold',\n",
              " 'hand',\n",
              " '”',\n",
              " '—',\n",
              " '50',\n",
              " '100',\n",
              " 'coin',\n",
              " '—',\n",
              " 'later',\n",
              " 'bringing',\n",
              " 'bag',\n",
              " 'containing',\n",
              " '$',\n",
              " '500',\n",
              " 'change',\n",
              " ',',\n",
              " 'affidavit',\n",
              " 'says.',\n",
              " 'The',\n",
              " 'source',\n",
              " 'reported',\n",
              " 'Haile',\n",
              " 'spent',\n",
              " '$',\n",
              " '13,050',\n",
              " 'October',\n",
              " ',',\n",
              " '$',\n",
              " '7,780',\n",
              " 'November',\n",
              " '$',\n",
              " '7,350',\n",
              " 'December.',\n",
              " 'Haile',\n",
              " 'lot.',\n",
              " 'Records',\n",
              " 'Virginia',\n",
              " 'Lottery',\n",
              " 'show',\n",
              " 'Haile',\n",
              " '$',\n",
              " '32,000',\n",
              " 'last',\n",
              " 'year',\n",
              " ',',\n",
              " '$',\n",
              " '17,000',\n",
              " '2010',\n",
              " ',',\n",
              " '$',\n",
              " '8,400',\n",
              " '2009',\n",
              " '$',\n",
              " '5,400',\n",
              " '2008',\n",
              " ',',\n",
              " 'according',\n",
              " 'affidavit.',\n",
              " 'The',\n",
              " 'document',\n",
              " 'said',\n",
              " 'informant',\n",
              " 'told',\n",
              " 'official',\n",
              " 'Haile',\n",
              " 'also',\n",
              " 'cashed',\n",
              " 'winning',\n",
              " 'ticket',\n",
              " 'le',\n",
              " '$',\n",
              " '600.',\n",
              " 'Records',\n",
              " '’',\n",
              " 'required',\n",
              " 'lesser',\n",
              " 'amounts.',\n",
              " 'Gaddis',\n",
              " 'wrote',\n",
              " 'lottery',\n",
              " 'official',\n",
              " 'said',\n",
              " 'Haile',\n",
              " '“',\n",
              " 'must',\n",
              " 'buying',\n",
              " 'extraordinary',\n",
              " 'amount',\n",
              " 'ticket',\n",
              " 'win',\n",
              " 'regularity',\n",
              " ',',\n",
              " 'often',\n",
              " 'multiple',\n",
              " 'time',\n",
              " 'month.',\n",
              " '”',\n",
              " 'Metro',\n",
              " 'chief',\n",
              " 'spokesman',\n",
              " 'Dan',\n",
              " 'Stessel',\n",
              " 'said',\n",
              " 'Thursday',\n",
              " 'two',\n",
              " 'men',\n",
              " 'suspended',\n",
              " 'without',\n",
              " 'pay',\n",
              " 'job',\n",
              " 'Metro',\n",
              " '“',\n",
              " 'pending',\n",
              " 'termination.',\n",
              " '”',\n",
              " 'Stessel',\n",
              " 'would',\n",
              " 'disclose',\n",
              " 'salary',\n",
              " ',',\n",
              " 'saying',\n",
              " 'Metro',\n",
              " 'release',\n",
              " 'information',\n",
              " 'executive',\n",
              " 'leadership',\n",
              " 'team.',\n",
              " 'McDade',\n",
              " 'worked',\n",
              " 'Metro',\n",
              " 'since',\n",
              " 'February',\n",
              " '1979.',\n",
              " 'In',\n",
              " '1989',\n",
              " ',',\n",
              " 'convicted',\n",
              " 'Prince',\n",
              " 'George',\n",
              " '’',\n",
              " 'County',\n",
              " 'two',\n",
              " 'count',\n",
              " 'possession',\n",
              " 'controlled',\n",
              " 'substance.',\n",
              " 'He',\n",
              " 'fined',\n",
              " '$',\n",
              " '500',\n",
              " 'sentenced',\n",
              " 'two',\n",
              " 'year',\n",
              " ',',\n",
              " 'suspended',\n",
              " ',',\n",
              " 'according',\n",
              " 'court',\n",
              " 'records.',\n",
              " 'Haile',\n",
              " 'worked',\n",
              " 'Metro',\n",
              " 'Transit',\n",
              " 'Police',\n",
              " 'since',\n",
              " 'August',\n",
              " '1997.',\n",
              " 'The',\n",
              " 'supervisor',\n",
              " 'revenue',\n",
              " 'facility',\n",
              " 'Alexandria',\n",
              " '“',\n",
              " 'relieved',\n",
              " 'duty',\n",
              " ',',\n",
              " '”',\n",
              " 'according',\n",
              " 'Metro',\n",
              " 'news',\n",
              " 'medium',\n",
              " 'release.',\n",
              " 'Stessel',\n",
              " 'would',\n",
              " 'identify',\n",
              " 'person',\n",
              " ',',\n",
              " 'saying',\n",
              " '“',\n",
              " 'action',\n",
              " 'personnel',\n",
              " 'matter',\n",
              " 'rather',\n",
              " 'criminal',\n",
              " 'matter.',\n",
              " '”',\n",
              " 'Metro',\n",
              " 'General',\n",
              " 'Manager',\n",
              " 'Richard',\n",
              " 'Sarles',\n",
              " 'said',\n",
              " 'Thursday',\n",
              " 'afternoon',\n",
              " '“',\n",
              " 'thoroughly',\n",
              " 'disgusted',\n",
              " 'dismayed',\n",
              " '”',\n",
              " 'McDade',\n",
              " 'Haile',\n",
              " '’',\n",
              " 'alleged',\n",
              " 'actions.',\n",
              " 'He',\n",
              " 'described',\n",
              " 'alleged',\n",
              " 'scheme',\n",
              " '“',\n",
              " 'betrayal',\n",
              " 'honest',\n",
              " 'employee',\n",
              " 'Washington',\n",
              " 'Metro.',\n",
              " '”',\n",
              " 'Stessel',\n",
              " 'said',\n",
              " 'Metro',\n",
              " 'collect',\n",
              " '$',\n",
              " '38',\n",
              " 'million',\n",
              " 'month',\n",
              " 'rail',\n",
              " 'fare',\n",
              " '—',\n",
              " '$',\n",
              " '18',\n",
              " 'million',\n",
              " 'cash',\n",
              " ',',\n",
              " 'rest',\n",
              " 'debit',\n",
              " 'credit',\n",
              " 'cards.',\n",
              " 'Collecting',\n",
              " 'money',\n",
              " 'tricky.',\n",
              " 'Metro',\n",
              " 'us',\n",
              " 'special',\n",
              " 'train',\n",
              " ',',\n",
              " 'known',\n",
              " '“',\n",
              " 'Money',\n",
              " 'Train',\n",
              " ',',\n",
              " '”',\n",
              " 'made',\n",
              " 'several',\n",
              " 'rail',\n",
              " 'car',\n",
              " 'run',\n",
              " 'night.',\n",
              " 'The',\n",
              " 'passenger',\n",
              " 'heavily',\n",
              " 'armed',\n",
              " 'Metro',\n",
              " 'Transit',\n",
              " 'Police',\n",
              " 'officer',\n",
              " 'technician',\n",
              " 'collect',\n",
              " 'money',\n",
              " 'large',\n",
              " 'silver',\n",
              " 'case',\n",
              " 'wheels.',\n",
              " 'The',\n",
              " 'train',\n",
              " 'taken',\n",
              " 'rail',\n",
              " 'yard',\n",
              " 'Alexandria',\n",
              " ',',\n",
              " 'facility',\n",
              " 'collect',\n",
              " 'sort',\n",
              " 'cash.',\n",
              " 'McDade',\n",
              " ',',\n",
              " 'revenue',\n",
              " 'technician',\n",
              " ',',\n",
              " 'part',\n",
              " 'special',\n",
              " 'team',\n",
              " 'supplement',\n",
              " 'Money',\n",
              " 'Train.',\n",
              " 'Known',\n",
              " '“',\n",
              " 'Emergencies',\n",
              " ',',\n",
              " '”',\n",
              " 'member',\n",
              " 'allowed',\n",
              " 'go',\n",
              " 'accompanied',\n",
              " 'single',\n",
              " 'Metro',\n",
              " 'Transit',\n",
              " 'Police',\n",
              " 'officer',\n",
              " '“',\n",
              " 'replenish',\n",
              " 'money',\n",
              " 'machine',\n",
              " 'retrieve',\n",
              " 'money',\n",
              " 'machine',\n",
              " 'full',\n",
              " ',',\n",
              " '”',\n",
              " 'according',\n",
              " 'affidavit.',\n",
              " 'Stessel',\n",
              " 'would',\n",
              " 'explain',\n",
              " 'Metro',\n",
              " 'reconciles',\n",
              " 'much',\n",
              " 'collected',\n",
              " 'fare',\n",
              " 'machine',\n",
              " 'taken',\n",
              " 'Alexandria',\n",
              " 'facility',\n",
              " ',',\n",
              " 'saying',\n",
              " ',',\n",
              " '“',\n",
              " 'Those',\n",
              " 'detail',\n",
              " 'pertain',\n",
              " 'ongoing',\n",
              " 'criminal',\n",
              " 'investigation.',\n",
              " '”',\n",
              " 'It',\n",
              " '’',\n",
              " 'first',\n",
              " 'time',\n",
              " 'worker',\n",
              " 'handling',\n",
              " 'Metro',\n",
              " '’',\n",
              " 'money',\n",
              " 'accused',\n",
              " 'stealing.',\n",
              " 'In',\n",
              " '2004',\n",
              " ',',\n",
              " 'Metro',\n",
              " 'switched',\n",
              " 'allowing',\n",
              " 'patron',\n",
              " 'pay',\n",
              " 'cash',\n",
              " 'parking',\n",
              " 'facility',\n",
              " 'requiring',\n",
              " 'use',\n",
              " 'electronic',\n",
              " 'SmarTrip',\n",
              " 'card',\n",
              " 'investigation',\n",
              " 'found',\n",
              " 'cashier',\n",
              " 'hired',\n",
              " 'contractor',\n",
              " 'stolen',\n",
              " 'million',\n",
              " 'dollar',\n",
              " 'revenue',\n",
              " 'several',\n",
              " 'years.',\n",
              " 'Sarles',\n",
              " 'said',\n",
              " 'transit',\n",
              " 'agency',\n",
              " '’',\n",
              " 'chief',\n",
              " 'financial',\n",
              " 'officer',\n",
              " '“',\n",
              " 'immediately',\n",
              " 'bring',\n",
              " 'forensic',\n",
              " 'accountant',\n",
              " 'conduct',\n",
              " 'thorough',\n",
              " 'review',\n",
              " 'control',\n",
              " 'system',\n",
              " 'management',\n",
              " 'revenue',\n",
              " 'system',\n",
              " 'fully',\n",
              " 'understand',\n",
              " 'wrongdoing',\n",
              " 'occurred',\n",
              " 'implement',\n",
              " 'tighter',\n",
              " 'detection',\n",
              " 'systems.',\n",
              " '”',\n",
              " 'The',\n",
              " 'transit',\n",
              " 'agency',\n",
              " '’',\n",
              " 'inspector',\n",
              " 'general',\n",
              " 'also',\n",
              " 'conducting',\n",
              " 'review',\n",
              " ',',\n",
              " 'Metro',\n",
              " 'Transit',\n",
              " 'Police',\n",
              " 'Chief',\n",
              " 'Michael',\n",
              " 'Taborn',\n",
              " 'expected',\n",
              " 'conduct',\n",
              " '“',\n",
              " 'top-to-bottom',\n",
              " 'assessment',\n",
              " 'policy',\n",
              " 'procedure',\n",
              " '”',\n",
              " 'involving',\n",
              " 'officer',\n",
              " 'assigned',\n",
              " 'revenue-collection',\n",
              " 'unit',\n",
              " ',',\n",
              " 'official',\n",
              " 'said.',\n",
              " 'The',\n",
              " 'case',\n",
              " 'come',\n",
              " 'Metro',\n",
              " 'considers',\n",
              " 'asking',\n",
              " 'rider',\n",
              " 'pay',\n",
              " 'higher',\n",
              " 'fare',\n",
              " 'parking',\n",
              " 'fee',\n",
              " 'help',\n",
              " 'fill',\n",
              " '$',\n",
              " '116',\n",
              " 'million',\n",
              " 'budget',\n",
              " 'gap',\n",
              " 'fiscal',\n",
              " '2013.',\n",
              " 'Riders',\n",
              " ',',\n",
              " 'already',\n",
              " 'stressed',\n",
              " 'service',\n",
              " 'delay',\n",
              " ',',\n",
              " 'chronic',\n",
              " 'equipment',\n",
              " 'failure',\n",
              " 'constant',\n",
              " 'maintenance',\n",
              " 'work',\n",
              " ',',\n",
              " 'unhappy',\n",
              " 'prospect.',\n",
              " 'Metro',\n",
              " 'worker',\n",
              " ',',\n",
              " 'angry',\n",
              " 'colleague',\n",
              " ',',\n",
              " 'worried',\n",
              " 'Thursday',\n",
              " 'alleged',\n",
              " 'theft',\n",
              " 'tarnish',\n",
              " 'transit',\n",
              " 'authority',\n",
              " '’',\n",
              " 'reputation.',\n",
              " '“',\n",
              " 'It',\n",
              " 'look',\n",
              " 'bad',\n",
              " 'u',\n",
              " 'front',\n",
              " 'line',\n",
              " ',',\n",
              " '”',\n",
              " 'said',\n",
              " 'train',\n",
              " 'operator',\n",
              " 'spoke',\n",
              " 'condition',\n",
              " 'anonymity',\n",
              " 'employee',\n",
              " 'allowed',\n",
              " 'talk',\n",
              " 'medium',\n",
              " 'without',\n",
              " 'permission.',\n",
              " '“',\n",
              " 'If',\n",
              " 'Metro',\n",
              " 'uniform',\n",
              " ',',\n",
              " 'people',\n",
              " 'already',\n",
              " 'say',\n",
              " 'thing',\n",
              " 'like',\n",
              " ':',\n",
              " '‘',\n",
              " 'You',\n",
              " '’',\n",
              " 'stupid.',\n",
              " 'When',\n",
              " 'going',\n",
              " 'finish',\n",
              " '?',\n",
              " '’',\n",
              " '“',\n",
              " 'This',\n",
              " 'give',\n",
              " 'negative',\n",
              " 'perception.',\n",
              " '”',\n",
              " 'Staff',\n",
              " 'researcher',\n",
              " 'Jennifer',\n",
              " 'Jenkins',\n",
              " 'contributed',\n",
              " 'report',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "Si29ZiZBSQHn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 3:\n",
        "Implement two similarity techniques. \n",
        "We would like to examine the Jacard Similarity and Cosine.\n",
        "\n",
        "Jacardian Similarity: here we want to identify the set of words in two documents that overlap and then divide that by the count of unique words across both documents.\n",
        "\n",
        "Cosine Similarity: Here we want to create vector representations for each document. Specifically, we want to come up with a vector that is based on the list of all words that occur across both documents. Then for each document we will create a vector that includes the counts of the number of time a word occurs in the document.\n",
        "\n",
        "So if the document 1 is: \"the ship sails at midnight\" and document 2 is: \"the crow flies at noon.\" We would be creating a vector like: [the, ship, sails, at, midnight, crow, flies, noon]. Then we would calculate the values of the vector for each document. For document 1: [1,1,1,1,1,0,0,0] and for document 2: [1,0,0,1,0,1,1,1]. With these two vectors we would simply take the dot product and that would provide the cosine similarity. \n"
      ]
    },
    {
      "metadata": {
        "id": "vrZcYH6fSQHp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#just to convert to string\n",
        "def stringify(whatever):\n",
        "    newString = \" \".join(whatever)\n",
        "    return newString\n",
        "\n",
        "#doc1 = stringify(processed_article_hash[key])\n",
        "#type(doc1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KCmptkNgSQHy",
        "colab_type": "code",
        "colab": {},
        "outputId": "7f58c1ce-d2a7-4d7f-9976-69337681c269"
      },
      "cell_type": "code",
      "source": [
        "def jacardian_distance(document_1_data, document_2_data):\n",
        "    doc1String = stringify(document_1_data)\n",
        "    doc2String = stringify(document_2_data)\n",
        "    words_in_doc_1_not_in_doc_2 = set(doc1String.split()) \n",
        "    words_in_doc_2_not_in_doc_1 = set(doc2String.split())\n",
        "    words_in_both_doc_1_and_doc_2 = words_in_doc_1_not_in_doc_2.intersection(words_in_doc_2_not_in_doc_1)\n",
        "    \n",
        "    jacardian = float(len(words_in_both_doc_1_and_doc_2)) / (len(words_in_doc_1_not_in_doc_2) + len(words_in_doc_2_not_in_doc_1) - len(words_in_both_doc_1_and_doc_2))\n",
        "    \n",
        "    return jacardian\n",
        "\n",
        "#test\n",
        "doc1=processed_article_hash['article_1']\n",
        "doc2=processed_article_hash['article_12']\n",
        "jacardian_distance(doc1, doc2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06680805938494168"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "EJrWv3sLSQIT",
        "colab_type": "code",
        "colab": {},
        "outputId": "a52e97f4-a64e-414c-f1f3-49697560a26c"
      },
      "cell_type": "code",
      "source": [
        "#no clue how to vectorize documents against the combined\n",
        "#def cosine_similarity(document_1_data, document_2_data):\n",
        "    #document_vector_word_index = [document_1_data + document_2_data]\n",
        "    #document_vector_word_index = list(set(document_vector_word_index))\n",
        "   # print(document_vector_word_index)\n",
        "    #document_1_vector = [] # fill in the array with the frequency of the words in the document\n",
        "    #document_2_vector = [] # fill in the array with the frequency of the words in the document\n",
        "\n",
        "    \n",
        "#adapted from https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings?rq=1\n",
        "\n",
        "def cosine_similarity(document_1_data, document_2_data):\n",
        "    WORD = re.compile(r'\\w+') \n",
        "    \n",
        "    def get_cosine(vec1, vec2):\n",
        "        intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "        numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "        sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
        "        sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
        "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "        \n",
        "        if not denominator:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return float(numerator) / denominator\n",
        "        \n",
        "    def text_to_vector(text):\n",
        "        words = WORD.findall(text)\n",
        "        return Counter(words)\n",
        "    \n",
        "    text1 = stringify(document_1_data)\n",
        "    text2 = stringify(document_2_data)\n",
        "    \n",
        "    vector1 = text_to_vector(text1)\n",
        "    vector2 = text_to_vector(text2)\n",
        "    \n",
        "    cosine = get_cosine(vector1, vector2)\n",
        "    \n",
        "    return(cosine)\n",
        "\n",
        "    \n",
        "#test   \n",
        "doc1=processed_article_hash['article_6']\n",
        "doc2=processed_article_hash['article_12']  \n",
        "cosine_similarity(doc1, doc2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16454108484552787"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "ujp7fKmESQIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 4:\n",
        "Now that we have our two similarity measures, we want to examine each document relative to each other and calculate their similarity. \n",
        "\n",
        "So we will want to create two tables that show the document similarities using both techniques."
      ]
    },
    {
      "metadata": {
        "id": "kEDNrtfySQIf",
        "colab_type": "code",
        "colab": {},
        "outputId": "00c91888-c856-43c4-af6f-841448cb9fc9"
      },
      "cell_type": "code",
      "source": [
        "#i have no idea how to do it this way\n",
        "# create a variable to store your table data... you could use a hash or some other data structure. \n",
        "# We just want it to identify which document is being compared to which other document.\n",
        "\n",
        "#data_structure_for_jacard_similarity = #\n",
        "#data_structure_for_cosine_similarity = #\n",
        "\n",
        "#for doc_1_key in article_hash.keys():\n",
        "   # for doc_2_key in article_hash.keys():\n",
        "        # we have the nested for loops as one way to compare each document to each other document\n",
        "     #   data_structure_for_jacard_similarity[doc_1_key][doc_2_key] = jacardian_distance(doc_1_processed_text, doc_2_processed_text)\n",
        "      #  data_structure_for_cosine_similarity[doc_1_key][doc_2_key] = jacardian_distance(doc_1_processed_text, doc_2_processed_text\n",
        "                                                                                       \n",
        "\n",
        "# finally, find some way to present this data back. Either as a straight table or a heatmap.\n",
        "\n",
        "df = pd.DataFrame()\n",
        "dataList = []\n",
        "for doc_1_key in article_hash.keys():\n",
        "   for doc_2_key in article_hash.keys():\n",
        "    temp = (doc_1_key, doc_2_key, jacardian_distance(processed_article_hash[doc_1_key],processed_article_hash[doc_2_key]),cosine_similarity(processed_article_hash[doc_1_key],processed_article_hash[doc_2_key]))\n",
        "    dataList.append(temp)\n",
        "\n",
        "\n",
        "#textfile.close()\n",
        "df=pd.DataFrame(dataList,columns=['First','Second','Jacard','Cosine'])\n",
        "df.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>First</th>\n",
              "      <th>Second</th>\n",
              "      <th>Jacard</th>\n",
              "      <th>Cosine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>article_1</td>\n",
              "      <td>article_1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>article_1</td>\n",
              "      <td>article_10</td>\n",
              "      <td>0.045296</td>\n",
              "      <td>0.068519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>article_1</td>\n",
              "      <td>article_11</td>\n",
              "      <td>0.034358</td>\n",
              "      <td>0.077635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>article_1</td>\n",
              "      <td>article_12</td>\n",
              "      <td>0.066808</td>\n",
              "      <td>0.125983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>article_1</td>\n",
              "      <td>article_2</td>\n",
              "      <td>0.134402</td>\n",
              "      <td>0.474544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       First      Second    Jacard    Cosine\n",
              "0  article_1   article_1  1.000000  1.000000\n",
              "1  article_1  article_10  0.045296  0.068519\n",
              "2  article_1  article_11  0.034358  0.077635\n",
              "3  article_1  article_12  0.066808  0.125983\n",
              "4  article_1   article_2  0.134402  0.474544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "uMn2OMQLSQIm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv('articleSimilarities.csv', sep=',', encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWNWKqh5SQIr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Step 5:\n",
        "You should now have two different similarity mechanisms. What do your results suggest? From perusing the documents, do you think the suggested ones are similar or not? Does tokenization, stemming, stop word removal or anything else improve your results?\n",
        "\n",
        "Write a brief description of your reactions to identifying these similar documents and what measures and pre-processing steps you think worked best."
      ]
    },
    {
      "metadata": {
        "id": "aOMP2Wg1SQIu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "JEC Response:\n",
        "\n",
        "In comparing these documents, I noticed most strongly that removing stop words moved the similarity in documents for most from around a .70-.80 to about a .05-.25 result in similarity with the highest achieving .28 in Jacard and .61 in Cosine. I personally feel that lemmatization might be better at capturing similarity in feel among documents, but I'm unsure if this is the correct approach. \n",
        "\n",
        "As mentioned above the .28 Jacard and .61 Cosine is between Article 11 and Article 10. These two articles are about the DC Metro and do share much similarity. In second with a Jacard of .16 and a Cosine of .42 are articles 5 and 6 which discuss Supreme Court Actions. While the articles vary about on what the Supreme Court is acting, they do have commonality. In the middle, Article 7 and 2 have a .19 cosine similarity, but these two documents are about the Superbowl and Chernobyl. Articles 7 and 4 with a .13 in Jacardian and a .27 in Cosine with the two articles discussing the Superbowl and the World Series victories which is at least in the realm of sports, but still differs. At the low end the documents are indeed strongly dissimilar. Overall, it seems cosine similarity is a better indicator of similarity for this exercise\n",
        "\n",
        "Joseph Conran\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3PO2odzfSQIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<-- put your comments here -->"
      ]
    }
  ]
}